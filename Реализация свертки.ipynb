{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4cbda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\__pycache__\\envs\\dsfs\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Создаем входной массив из двух изображений RGB 3*3\n",
    "input_images = torch.tensor(\n",
    "      [[[[0,  1,  2],\n",
    "         [3,  4,  5],\n",
    "         [6,  7,  8]],\n",
    "\n",
    "        [[9, 10, 11],\n",
    "         [12, 13, 14],\n",
    "         [15, 16, 17]],\n",
    "\n",
    "        [[18, 19, 20],\n",
    "         [21, 22, 23],\n",
    "         [24, 25, 26]]],\n",
    "\n",
    "\n",
    "       [[[27, 28, 29],\n",
    "         [30, 31, 32],\n",
    "         [33, 34, 35]],\n",
    "\n",
    "        [[36, 37, 38],\n",
    "         [39, 40, 41],\n",
    "         [42, 43, 44]],\n",
    "\n",
    "        [[45, 46, 47],\n",
    "         [48, 49, 50],\n",
    "         [51, 52, 53]]]])\n",
    "\n",
    "\n",
    "def get_padding2d(input_images):\n",
    "    padded_images = torch.nn.functional.pad(input_images,(1,1,1,1))# добавить нулей с четырех сторон каждого изображения\n",
    "    return padded_images.float()\n",
    "\n",
    "\n",
    "correct_padded_images = torch.tensor(\n",
    "       [[[[0.,  0.,  0.,  0.,  0.],\n",
    "          [0.,  0.,  1.,  2.,  0.],\n",
    "          [0.,  3.,  4.,  5.,  0.],\n",
    "          [0.,  6.,  7.,  8.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0.,  9., 10., 11.,  0.],\n",
    "          [0., 12., 13., 14.,  0.],\n",
    "          [0., 15., 16., 17.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 18., 19., 20.,  0.],\n",
    "          [0., 21., 22., 23.,  0.],\n",
    "          [0., 24., 25., 26.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]]],\n",
    "\n",
    "\n",
    "        [[[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 27., 28., 29.,  0.],\n",
    "          [0., 30., 31., 32.,  0.],\n",
    "          [0., 33., 34., 35.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 36., 37., 38.,  0.],\n",
    "          [0., 39., 40., 41.,  0.],\n",
    "          [0., 42., 43., 44.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]],\n",
    "\n",
    "         [[0.,  0.,  0.,  0.,  0.],\n",
    "          [0., 45., 46., 47.,  0.],\n",
    "          [0., 48., 49., 50.,  0.],\n",
    "          [0., 51., 52., 53.,  0.],\n",
    "          [0.,  0.,  0.,  0.,  0.]]]])\n",
    "\n",
    "\n",
    "print(torch.allclose(get_padding2d(input_images), correct_padded_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edfa6810",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bbee0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
    "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
    "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "    return batch_size, out_channels, output_height, output_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296e4df",
   "metadata": {},
   "source": [
    "# Реализация свертки через цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9c8e2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[[[] for _ in range(row)] for _ in range(slow)]for _ in range(batch_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0a32e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1493., 1327.],\n",
       "          [1358., 1282.]]],\n",
       "\n",
       "\n",
       "        [[[3788., 3163.],\n",
       "          [3113., 2848.]]]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a452efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stri = 2\n",
    "size = kernel.size()[3]\n",
    "for i in range(batch_s):\n",
    "    for j in range(slow):\n",
    "        for t in range(row):\n",
    "            for y in range(col):\n",
    "                cl = 0\n",
    "                if (y*stri or t*stri) <= correct_padded_images.size()[3] - size:\n",
    "                    for l in range(kernel.size()[1]):\n",
    "                        cl += (correct_padded_images[i][l][t*stri:size+stri*t,y*stri:size+stri*y] * kernel[j][l]).sum()\n",
    "                \n",
    "                    res[i][j][t].append(cl)\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121a0e31",
   "metadata": {},
   "source": [
    "# Реализация свертки через матрицы первый способ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df85c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s,chanel,row,col = calc_out_shape(list(correct_padded_images.size()),1,3,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af1dc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ten,size_ker = correct_padded_images.size()[2],kernel.size()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "512789ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(row*row,size_ten*size_ten*size_ker)\n",
    "stride = 2\n",
    "count = 0 \n",
    "for j in range(size_ker):\n",
    "    col_l,col_r,row_u,row_d = 0,size_ten-size_ker,0,size_ten-size_ker\n",
    "    for i in range(row*row):\n",
    "        if i%row==0 and i!=0:\n",
    "            row_u += stride\n",
    "            row_d -= stride\n",
    "            col_l,col_r = 0,size_ten-size_ker\n",
    "            t[i,j*size_ten*size_ten:size_ten*size_ten*(j+1)] = torch.flatten(torch.nn.functional.pad(kernel[0][j],(col_l,col_r,row_u,row_d)),0)\n",
    "        else:\n",
    "            t[i,j*size_ten*size_ten:size_ten*size_ten*(j+1)] = torch.flatten(torch.nn.functional.pad(kernel[0][j],(col_l,col_r,row_u,row_d)),0)\n",
    "        col_l += stride\n",
    "        col_r -= stride\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91eccf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1493., 3788.],\n",
       "          [1327., 3163.]]],\n",
       "\n",
       "\n",
       "        [[[1358., 3113.],\n",
       "          [1282., 2848.]]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t @ correct_padded_images.view(2,-1).permute(1,0)).view(batch_s,chanel,row,col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df99169",
   "metadata": {},
   "source": [
    "# Реализация свертки через  матрицы второй способ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30bd2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_s,chanel,row,col = calc_out_shape(list(correct_padded_images.size()),1,3,2,0)\n",
    "size_ten,size_ker,size_chan = correct_padded_images.size()[2],kernel.size()[2],correct_padded_images.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8a6e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ker = torch.flatten(kernel,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8859b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_res = torch.zeros(size_chanel*size_ker*size_ker,row*row*batch_s)\n",
    "strok = 0\n",
    "stride = 2\n",
    "for ten in range(batch_s):\n",
    "    for i in range(size_chanel):\n",
    "        for j in range(row*row):\n",
    "            if j%row == 0 and j!=0:\n",
    "                strok += stride\n",
    "            torch_res[i*size_ker*size_ker:size_ker*size_ker*(i+1),j+(ten*row*row)]\\\n",
    "            = torch.flatten(correct_padded_images[ten][i][strok:size_ker+strok,j*stride-(strok*stride):(size_ker+stride*j)-(strok*stride)],0)\n",
    "        strok = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97508cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1493., 1327.],\n",
       "          [1358., 1282.]]],\n",
       "\n",
       "\n",
       "        [[[3788., 3163.],\n",
       "          [3113., 2848.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conv_ker @ torch_res).transpose(1,0).view(batch_s,chanel,row,col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
